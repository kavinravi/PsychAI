{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76122604",
   "metadata": {},
   "source": [
    "ONLY RUN THE BELOW CELL IF IN GOOGLE COLAB\n",
    "\n",
    "1. un-comment all of the code\n",
    "2. TO MAKE IT FASTER: go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "3. click run, it should print out all the packages and versions\n",
    "\n",
    "real notebook starts beneath this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584baeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import sys, subprocess\n",
    "\n",
    "# try:\n",
    "#     import torch\n",
    "#     print(\"Found torch:\", torch.__version__, \"CUDA:\", getattr(torch.version, \"cuda\", None))\n",
    "# except Exception:\n",
    "#     subprocess.check_call([\n",
    "#         sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
    "#         \"torch\", \"torchvision\", \"torchaudio\",\n",
    "#         \"--index-url\", \"https://download.pytorch.org/whl/cu121\"\n",
    "#     ])\n",
    "\n",
    "# pkgs = [\n",
    "#     \"transformers>=4.43.3\",\n",
    "#     \"peft>=0.12.0\",\n",
    "#     \"trl>=0.9.6\",\n",
    "#     \"accelerate>=0.33.0\",\n",
    "#     \"datasets>=2.19.0\",\n",
    "#     \"bitsandbytes>=0.43.0\",\n",
    "#     \"evaluate>=0.4.1\",\n",
    "#     \"safetensors>=0.4.3\",\n",
    "#     \"huggingface_hub>=0.23.0\",\n",
    "#     \"sentencepiece>=0.2.0\",\n",
    "#     \"tqdm>=4.66\",\n",
    "#     \"pandas>=2.2\",\n",
    "#     \"numpy>=1.26\",\n",
    "#     \"python-dotenv>=1.0.1\",\n",
    "#     \"google-generativeai>=0.7.0\",\n",
    "#     \"tqdm>=4.66.3\",\n",
    "# ]\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "# import torch, transformers, peft, accelerate, datasets, bitsandbytes as bnb, trl\n",
    "# print(\"torch:\", torch.__version__, \"cuda:\", getattr(torch.version, \"cuda\", None))\n",
    "# print(\"transformers:\", transformers.__version__)\n",
    "# print(\"peft:\", peft.__version__)\n",
    "# print(\"trl:\", trl.__version__)\n",
    "# print(\"accelerate:\", accelerate.__version__)\n",
    "# print(\"datasets:\", datasets.__version__)\n",
    "# print(\"bitsandbytes:\", getattr(bnb, \"__version__\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9cd54",
   "metadata": {},
   "source": [
    "# Gathering & Anonymizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b4f65",
   "metadata": {},
   "source": [
    "## Data Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb78306",
   "metadata": {},
   "source": [
    "1. [counselor/participant Q&A pseudo-chats](https://github.com/nbertagnolli/counsel-chat)\n",
    "2. [HOPE Dataset (I filled out access request form)](https://github.com/LCS2-IIITD/SPARTA_WSDM2022/tree/main#hope-dataset-access-request)\n",
    "3. [APA link for videos, needs free trial](https://www.ebsco.com/products/research-databases/apa-psyctherapy#:~:text=APA%20PsycTherapy%20,repository%20of%20therapy%20videos)\n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1d88d",
   "metadata": {},
   "source": [
    "### Set 1: Counselor/participant Q&A pseudo-chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11ffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Filter: 100%|██████████| 2775/2775 [00:00<00:00, 40704.17 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mclassify\u001b[39m\u001b[34m(example)\u001b[39m\n\u001b[32m     55\u001b[39m txt = resp.text.strip()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m screened = []\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m stage1:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     lab = \u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     ex = \u001b[38;5;28mdict\u001b[39m(ex)\n\u001b[32m     67\u001b[39m     ex[\u001b[33m\"\u001b[39m\u001b[33m_screen\u001b[39m\u001b[33m\"\u001b[39m] = lab\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mclassify\u001b[39m\u001b[34m(example)\u001b[39m\n\u001b[32m     57\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mis_child_context\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mexclude_reason\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mllm_error\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrisk_flags\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mquality\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import re, json, time, os\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Stage 1: heuristics ----------\n",
    "ALLOW = re.compile(\n",
    "    r\"\\b(my|our)\\s+(son|daughter|child)\\b|\"\n",
    "    r\"\\b(teen|teenager|toddler|adolescent|pediatric|student|classmate|teacher|school|bully|bullying|iep|kindergarten|middle school|high school)\\b|\"\n",
    "    r\"\\b(aged|age)\\s*(\\d{1,2})\\s*(yo|year[- ]old)\\b\",\n",
    "    re.I\n",
    ")\n",
    "BLOCK = re.compile(\n",
    "    r\"\\b(erectile|ed\\b|libido|porn|orgasm|erection|ejaculat|sex(?!ual assault)|marital|affair|girlfriend|boyfriend|wife|husband)\\b\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "def stage1_pass(q):\n",
    "    if not q: return False\n",
    "    if BLOCK.search(q): return False\n",
    "    return bool(ALLOW.search(q))\n",
    "\n",
    "ds = load_dataset(\"nbertagnolli/counsel-chat\")[\"train\"]\n",
    "stage1 = ds.filter(lambda r: stage1_pass((r.get(\"questionText\") or \"\")))\n",
    "\n",
    "print(f\"After stage 1 filtering: {len(stage1)} examples\")\n",
    "\n",
    "# ---------- Stage 2: LLM screen (Gemini) ----------\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"GEMINI_MODEL\")\n",
    "\n",
    "if not API_KEY or not MODEL_NAME:\n",
    "    raise ValueError(\"Missing GEMINI_API_KEY or GEMINI_MODEL in .env file\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# Test API connection first\n",
    "try:\n",
    "    test_resp = genai.GenerativeModel(MODEL_NAME).generate_content(\"Hello\")\n",
    "    print(f\"API test successful: {test_resp.text[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"API test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "INSTRUCTIONS = \"\"\"You are a data screener for training a child-psychology assistant.\n",
    "Return strict JSON with keys: is_child_context (bool), exclude_reason (string), risk_flags (list of strings),\n",
    "quality (high|medium|low). Exclude adult-only sexual or couples topics, ED, etc.\n",
    "\"\"\"\n",
    "\n",
    "def classify(example):\n",
    "    q = (example.get(\"questionText\") or \"\").strip()\n",
    "    prompt = f\"\"\"\n",
    "{INSTRUCTIONS}\n",
    "\n",
    "QUESTION:\n",
    "{q}\n",
    "\n",
    "JSON ONLY:\n",
    "\"\"\"\n",
    "    \n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = genai.GenerativeModel(MODEL_NAME).generate_content(prompt)\n",
    "            txt = resp.text.strip()\n",
    "            data = json.loads(txt)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"API error (attempt {attempt + 1}/3): {e}\")\n",
    "            if attempt < 2:  # Don't sleep on last attempt\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    return {\"is_child_context\": False, \"exclude_reason\": \"llm_error\", \"risk_flags\": [], \"quality\": \"low\"}\n",
    "\n",
    "# Process with progress bar and rate limiting\n",
    "screened = []\n",
    "failed_count = 0\n",
    "\n",
    "for i, ex in enumerate(tqdm(stage1, desc=\"LLM Screening\")):\n",
    "    lab = classify(ex)\n",
    "    \n",
    "    if lab.get(\"exclude_reason\") == \"llm_error\":\n",
    "        failed_count += 1\n",
    "    \n",
    "    ex = dict(ex)\n",
    "    ex[\"_screen\"] = lab\n",
    "    screened.append(ex)\n",
    "    \n",
    "    # Rate limiting - adjust as needed\n",
    "    time.sleep(0.5)  # Add consistent rate limiting\n",
    "    \n",
    "    # Optional: Save progress periodically\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(stage1)}, Failed: {failed_count}\")\n",
    "\n",
    "print(f\"LLM screening complete. Failed requests: {failed_count}\")\n",
    "\n",
    "# Keep only high-signal child items\n",
    "final = [\n",
    "    ex for ex in screened\n",
    "    if ex[\"_screen\"].get(\"is_child_context\") is True\n",
    "       and ex[\"_screen\"].get(\"quality\") in (\"high\",\"medium\")\n",
    "       and ex[\"_screen\"].get(\"exclude_reason\") in (\"none\", \"\")\n",
    "]\n",
    "\n",
    "print(f\"Final dataset: {len(final)} examples\")\n",
    "\n",
    "# Save chat JSONL\n",
    "def scrub(s):\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"\\s+\",\" \",str(s)).strip()\n",
    "    s = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\",\"[redacted_email]\",s)\n",
    "    s = re.sub(r\"(https?://\\S+)\",\"[link]\",s)\n",
    "    s = re.sub(r\"\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\",\"[phone]\",s)\n",
    "    return s\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"data/1\", exist_ok=True)\n",
    "\n",
    "out_path = \"data/1/counselchat_child_subset_chat_screened.jsonl\" # in colab this would be /content/counselchat...jsonl\n",
    "with open(out_path,\"w\",encoding=\"utf-8\") as f:\n",
    "    seen = set()\n",
    "    for r in final:\n",
    "        q = scrub(r.get(\"questionText\",\"\"))\n",
    "        a = scrub(r.get(\"answerText\",\"\"))\n",
    "        key = (q,a)\n",
    "        if not q or not a or key in seen: \n",
    "            continue\n",
    "        seen.add(key)\n",
    "        f.write(json.dumps({\"messages\":[\n",
    "            {\"role\":\"user\",\"content\": q},\n",
    "            {\"role\":\"assistant\",\"content\": a}\n",
    "        ]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {out_path} with {len(seen)} examples\")\n",
    "print(f\"Summary: {len(ds)} -> {len(stage1)} -> {len(screened)} -> {len(final)} -> {len(seen)} unique examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11007fe1",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3",
   "language": "python",
   "name": "py312-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
