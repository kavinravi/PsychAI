{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76122604",
   "metadata": {},
   "source": [
    "ONLY RUN THE BELOW CELL IF IN GOOGLE COLAB\n",
    "\n",
    "1. un-comment all of the code\n",
    "2. TO MAKE IT FASTER: go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
    "3. click run, it should print out all the packages and versions\n",
    "\n",
    "real notebook starts beneath this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584baeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import sys, subprocess\n",
    "\n",
    "# try:\n",
    "#     import torch\n",
    "#     print(\"Found torch:\", torch.__version__, \"CUDA:\", getattr(torch.version, \"cuda\", None))\n",
    "# except Exception:\n",
    "#     subprocess.check_call([\n",
    "#         sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\",\n",
    "#         \"torch\", \"torchvision\", \"torchaudio\",\n",
    "#         \"--index-url\", \"https://download.pytorch.org/whl/cu121\"\n",
    "#     ])\n",
    "\n",
    "# pkgs = [\n",
    "#     \"transformers>=4.43.3\",\n",
    "#     \"peft>=0.12.0\",\n",
    "#     \"trl>=0.9.6\",\n",
    "#     \"accelerate>=0.33.0\",\n",
    "#     \"datasets>=2.19.0\",\n",
    "#     \"bitsandbytes>=0.43.0\",\n",
    "#     \"evaluate>=0.4.1\",\n",
    "#     \"safetensors>=0.4.3\",\n",
    "#     \"huggingface_hub>=0.23.0\",\n",
    "#     \"sentencepiece>=0.2.0\",\n",
    "#     \"tqdm>=4.66\",\n",
    "#     \"pandas>=2.2\",\n",
    "#     \"numpy>=1.26\",\n",
    "#     \"python-dotenv>=1.0.1\",\n",
    "#     \"google-generativeai>=0.7.0\",\n",
    "#     \"tqdm>=4.66.3\",\n",
    "# ]\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "# import torch, transformers, peft, accelerate, datasets, bitsandbytes as bnb, trl\n",
    "# print(\"torch:\", torch.__version__, \"cuda:\", getattr(torch.version, \"cuda\", None))\n",
    "# print(\"transformers:\", transformers.__version__)\n",
    "# print(\"peft:\", peft.__version__)\n",
    "# print(\"trl:\", trl.__version__)\n",
    "# print(\"accelerate:\", accelerate.__version__)\n",
    "# print(\"datasets:\", datasets.__version__)\n",
    "# print(\"bitsandbytes:\", getattr(bnb, \"__version__\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9cd54",
   "metadata": {},
   "source": [
    "# Gathering & Anonymizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b4f65",
   "metadata": {},
   "source": [
    "## Data Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb78306",
   "metadata": {},
   "source": [
    "1. [counselor/participant Q&A pseudo-chats](https://github.com/nbertagnolli/counsel-chat)\n",
    "2. [HOPE Dataset (I filled out access request form)](https://github.com/LCS2-IIITD/SPARTA_WSDM2022/tree/main#hope-dataset-access-request)\n",
    "3. [APA link for videos, needs free trial](https://www.ebsco.com/products/research-databases/apa-psyctherapy#:~:text=APA%20PsycTherapy%20,repository%20of%20therapy%20videos)\n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1d88d",
   "metadata": {},
   "source": [
    "### Set 1: Counselor/participant Q&A pseudo-chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini-screened child-context filter with robust JSON + sane acceptance\n",
    "# Requires: google-generativeai>=0.7.0, python-dotenv>=1.0.1, datasets, tqdm\n",
    "\n",
    "import re, json, time, os, random\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------- CONFIG ----------\n",
    "RATE_LIMIT_SEC = 1.0            # pause between calls\n",
    "MAX_ITEMS = None                # set int to cap API spend; None = all\n",
    "OUT_DIR = \"data/1\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Stage 1: BLOCKLIST-ONLY heuristic ----------\n",
    "BLOCK = re.compile(\n",
    "    r\"\\b(erectile|ed\\b|libido|porn|orgasm|erection|ejaculat|sex(?!ual assault)|\"\n",
    "    r\"marital|affair|wife|husband)\\b\",\n",
    "    re.I\n",
    ")\n",
    "def stage1_keep(q: str) -> bool:\n",
    "    q = q or \"\"\n",
    "    return not bool(BLOCK.search(q))\n",
    "\n",
    "ds = load_dataset(\"nbertagnolli/counsel-chat\")[\"train\"]\n",
    "\n",
    "kept_stage1, rejected_stage1 = [], []\n",
    "for ex in ds:\n",
    "    q = ex.get(\"questionText\") or \"\"\n",
    "    (kept_stage1 if stage1_keep(q) else rejected_stage1).append(ex)\n",
    "\n",
    "print(f\"Stage 1 — kept: {len(kept_stage1)} | rejected: {len(rejected_stage1)} | total: {len(ds)}\")\n",
    "\n",
    "if MAX_ITEMS is not None and len(kept_stage1) > MAX_ITEMS:\n",
    "    random.seed(3619)\n",
    "    kept_stage1 = random.sample(kept_stage1, MAX_ITEMS)\n",
    "    print(f\"Capped Stage 1 kept to {len(kept_stage1)} items (MAX_ITEMS).\")\n",
    "\n",
    "# ---------- Stage 2: LLM screen (Gemini JSON mode) ----------\n",
    "load_dotenv(override=True)\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"GEMINI_MODEL\")\n",
    "if not API_KEY or not MODEL_NAME:\n",
    "    raise ValueError(\"Missing GEMINI_API_KEY or GEMINI_MODEL in .env\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = genai.GenerativeModel(\n",
    "    MODEL_NAME,\n",
    "    generation_config={\n",
    "        \"temperature\": 0,\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "    },\n",
    "    system_instruction=(\n",
    "        \"You are a data screener for training a child-psychology assistant. \"\n",
    "        \"Return ONLY valid JSON matching exactly this schema:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"is_child_context\": <bool>,\\n'\n",
    "        '  \"exclude_reason\": \"<string>\",  // one of: \"none\",\"adult_sexual_topic\",\"general_adult\",\"off_topic\",\"unsafe\",\"pii_leak\"\\n'\n",
    "        '  \"risk_flags\": [\"<string>\", ...],\\n'\n",
    "        '  \"quality\": \"<string>\"          // one of: \"high\",\"medium\",\"low\"\\n'\n",
    "        \"}\\n\"\n",
    "        'If content is acceptable for a child/parent/teacher context, set is_child_context=true and exclude_reason=\"none\". '\n",
    "        \"Return JSON only, no extra text.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "INSTRUCTIONS = (\n",
    "    \"Return JSON only. Keys exactly: is_child_context (bool), exclude_reason (string), \"\n",
    "    \"risk_flags (list[string]), quality (string: high|medium|low).\"\n",
    ")\n",
    "\n",
    "def _extract_text(resp) -> str:\n",
    "    try:\n",
    "        if resp and getattr(resp, \"text\", None):\n",
    "            return resp.text\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: stitch parts if needed\n",
    "    try:\n",
    "        texts = []\n",
    "        for c in getattr(resp, \"candidates\", []) or []:\n",
    "            content = getattr(c, \"content\", None)\n",
    "            parts = getattr(content, \"parts\", []) if content else []\n",
    "            for p in parts or []:\n",
    "                t = getattr(p, \"text\", None)\n",
    "                if t: texts.append(t)\n",
    "        return \"\\n\".join(texts)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _safe_json_parse(txt: str):\n",
    "    if not txt:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(0))\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "# --- Normalizers to avoid \"0 kept\" issues ---\n",
    "def as_bool(x):\n",
    "    if x is True: return True\n",
    "    if x is False: return False\n",
    "    if isinstance(x, str): return x.strip().lower() in (\"true\",\"yes\",\"y\",\"1\")\n",
    "    if isinstance(x, (int, float)): return x != 0\n",
    "    return False\n",
    "\n",
    "def norm_str(x, default=\"\"):\n",
    "    if x is None: return default\n",
    "    return str(x).strip().lower()\n",
    "\n",
    "ACCEPT_REASONS = {\"\", \"none\", \"ok\", \"pass\"}\n",
    "GOOD_QUALITY   = {\"high\", \"medium\", \"\"}  # treat missing as medium\n",
    "\n",
    "def is_acceptable(lab: dict) -> bool:\n",
    "    return (\n",
    "        as_bool(lab.get(\"is_child_context\"))\n",
    "        and norm_str(lab.get(\"exclude_reason\")) in ACCEPT_REASONS\n",
    "        and norm_str(lab.get(\"quality\"), \"medium\") in GOOD_QUALITY\n",
    "    )\n",
    "\n",
    "JSON_FALLBACK = {\n",
    "    \"is_child_context\": False,\n",
    "    \"exclude_reason\": \"llm_error\",\n",
    "    \"risk_flags\": [],\n",
    "    \"quality\": \"low\",\n",
    "}\n",
    "\n",
    "def classify(ex):\n",
    "    q = (ex.get(\"questionText\") or \"\").strip()\n",
    "    prompt = f\"{INSTRUCTIONS}\\n\\nQUESTION:\\n{q}\\n\\nJSON ONLY:\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = model.generate_content(prompt)\n",
    "            txt = (_extract_text(resp) or \"\").strip()\n",
    "            data = _safe_json_parse(txt)\n",
    "            if isinstance(data, dict) and \"is_child_context\" in data:\n",
    "                return data\n",
    "            raise ValueError(\"non-json or missing keys\")\n",
    "        except Exception as e:\n",
    "            if attempt < 2:\n",
    "                time.sleep(2 ** attempt)  # 1s, 2s\n",
    "    return JSON_FALLBACK.copy()\n",
    "\n",
    "screened, stage2_rejected = [], []\n",
    "failed_count = 0\n",
    "\n",
    "for i, ex in enumerate(tqdm(kept_stage1, desc=\"Stage 2 LLM Screening\")):\n",
    "    lab = classify(ex)\n",
    "    if norm_str(lab.get(\"exclude_reason\")) == \"llm_error\":\n",
    "        failed_count += 1\n",
    "\n",
    "    ex2 = dict(ex); ex2[\"_screen\"] = lab\n",
    "    screened.append(ex2)\n",
    "\n",
    "    if not is_acceptable(lab):\n",
    "        stage2_rejected.append(ex2)\n",
    "\n",
    "    time.sleep(RATE_LIMIT_SEC)\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(f\"Processed {i+1}/{len(kept_stage1)} | LLM failures: {failed_count}\")\n",
    "\n",
    "print(f\"LLM screening complete. LLM failures: {failed_count}\")\n",
    "\n",
    "# Quick histograms so you can see what the model returned\n",
    "ex_reasons = Counter(norm_str(ex[\"_screen\"].get(\"exclude_reason\")) for ex in screened)\n",
    "ex_quality = Counter(norm_str(ex[\"_screen\"].get(\"quality\",\"medium\")) for ex in screened)\n",
    "ex_child   = Counter(as_bool(ex[\"_screen\"].get(\"is_child_context\")) for ex in screened)\n",
    "print(\"exclude_reason:\", ex_reasons)\n",
    "print(\"quality:\", ex_quality)\n",
    "print(\"is_child_context:\", ex_child)\n",
    "\n",
    "# ---------- Final keepers ----------\n",
    "final = [ex for ex in screened if is_acceptable(ex[\"_screen\"])]\n",
    "print(f\"Stage 2 — kept: {len(final)} | rejected: {len(screened) - len(final)}\")\n",
    "\n",
    "# ---------- Save outputs ----------\n",
    "def to_jsonl(path, rows):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "to_jsonl(os.path.join(OUT_DIR, \"rejected_stage1.jsonl\"), rejected_stage1)\n",
    "to_jsonl(os.path.join(OUT_DIR, \"rejected_stage2.jsonl\"), stage2_rejected)\n",
    "\n",
    "# Chat JSONL for training\n",
    "def scrub(s):\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"\\s+\",\" \",str(s)).strip()\n",
    "    s = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\",\"[redacted_email]\",s)\n",
    "    s = re.sub(r\"(https?://\\S+)\",\"[link]\",s)\n",
    "    s = re.sub(r\"\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\",\"[phone]\",s)\n",
    "    return s\n",
    "\n",
    "out_path = os.path.join(OUT_DIR, \"counselchat_child_subset_chat_screened.jsonl\")  # fixed path\n",
    "seen = set()\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in final:\n",
    "        q = scrub(r.get(\"questionText\",\"\")); a = scrub(r.get(\"answerText\",\"\"))\n",
    "        key = (q,a)\n",
    "        if not q or not a or key in seen: \n",
    "            continue\n",
    "        seen.add(key)\n",
    "        f.write(json.dumps({\"messages\":[\n",
    "            {\"role\":\"user\",\"content\": q},\n",
    "            {\"role\":\"assistant\",\"content\": a}\n",
    "        ]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {out_path} with {len(seen)} examples\")\n",
    "print(f\"Summary: total {len(ds)} -> stage1_kept {len(kept_stage1)} -> screened {len(screened)} -> final {len(final)} -> unique {len(seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557349bc",
   "metadata": {},
   "source": [
    "#### Local Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Local LLM screening with .env-driven config (using Qwen) ---\n",
    "# Prereqs (run once in another cell if needed):\n",
    "# !pip install -U torch --index-url https://download.pytorch.org/whl/cu128\n",
    "# !pip install -U \"transformers>=4.43.3\" \"accelerate>=0.33.0\" \"bitsandbytes>=0.43.0\" \"datasets>=2.19.0\" \"tqdm>=4.66\" \"python-dotenv>=1.0.1\"\n",
    "\n",
    "import os, re, json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# --- Load .env ---\n",
    "load_dotenv()\n",
    "\n",
    "def _as_int(name, default):\n",
    "    v = os.getenv(name, str(default)).strip()\n",
    "    try: return int(v)\n",
    "    except: return default\n",
    "\n",
    "def _as_bool(name, default=False):\n",
    "    v = os.getenv(name)\n",
    "    if v is None: return default\n",
    "    return v.strip().lower() in (\"1\",\"true\",\"yes\",\"y\",\"on\")\n",
    "\n",
    "def _as_choice(name, allowed, default):\n",
    "    v = os.getenv(name, default).strip().lower()\n",
    "    return v if v in allowed else default\n",
    "\n",
    "# --- Config from .env (with defaults) ---\n",
    "# Using HuggingFace model ID instead of local path\n",
    "MODEL_ID       = os.getenv(\"MODEL_ID\", \"Qwen/Qwen2.5-7B-Instruct\")\n",
    "OUT_DIR        = os.path.expanduser(os.getenv(\"OUT_DIR\",   \"data/1\"))\n",
    "BATCH_SIZE     = _as_int(\"BATCH_SIZE\", 8)                   # 7B model, slightly smaller batch\n",
    "MAX_NEW_TOKENS = _as_int(\"MAX_NEW_TOKENS\", 128)            # Increased for better JSON responses\n",
    "QUANT_MODE     = _as_choice(\"QUANT_MODE\", {\"8bit\",\"4bit\"}, \"8bit\")   # \"8bit\" | \"4bit\"\n",
    "DATASET_NAME   = os.getenv(\"HF_DATASET\", \"nbertagnolli/counsel-chat\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Using MODEL_ID={MODEL_ID}\")\n",
    "print(f\"OUT_DIR={OUT_DIR} | QUANT_MODE={QUANT_MODE} | BATCH_SIZE={BATCH_SIZE} | MAX_NEW_TOKENS={MAX_NEW_TOKENS}\")\n",
    "print(f\"HF_DATASET={DATASET_NAME}\")\n",
    "\n",
    "# --- Stage 1: blocklist-only (maximize recall; filter obvious adult-only topics) ---\n",
    "BLOCK = re.compile(\n",
    "    r\"\\b(erectile|ed\\b|libido|porn|orgasm|erection|ejaculat|sex(?!ual assault)|\"\n",
    "    r\"marital|affair|girlfriend|boyfriend|wife|husband)\\b\",\n",
    "    re.I\n",
    ")\n",
    "def stage1_keep(q: str) -> bool:\n",
    "    q = q or \"\"\n",
    "    return not bool(BLOCK.search(q))\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(DATASET_NAME)[\"train\"]\n",
    "kept_stage1, rejected_stage1 = [], []\n",
    "for ex in ds:\n",
    "    (kept_stage1 if stage1_keep(ex.get(\"questionText\") or \"\") else rejected_stage1).append(ex)\n",
    "print(f\"Stage 1 — kept: {len(kept_stage1)} | rejected: {len(rejected_stage1)} | total: {len(ds)}\")\n",
    "\n",
    "# --- Load Qwen model with requested quantization ---\n",
    "print(f\"\\nLoading {MODEL_ID}...\")\n",
    "if QUANT_MODE == \"8bit\":\n",
    "    bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "else:\n",
    "    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "# Some models don't have pad_token set\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# --- Prompt formatted for Qwen instruction following ---\n",
    "def make_prompt(q: str) -> str:\n",
    "    # Qwen works well with clear, structured instructions\n",
    "    system_msg = (\n",
    "        \"You are a data screener for training a child-psychology assistant. \"\n",
    "        \"Analyze the question and return ONLY a JSON object with these exact keys:\\n\"\n",
    "        \"- is_child_context (boolean): true if appropriate for child/family counseling\\n\"\n",
    "        \"- exclude_reason (string): reason for exclusion or 'none' if included\\n\"\n",
    "        \"- risk_flags (array): list of concerning elements if any\\n\"\n",
    "        \"- quality (string): 'high', 'medium', or 'low'\\n\\n\"\n",
    "        \"Exclude adult-only sexual topics, erectile dysfunction, marital/couples counseling, etc.\"\n",
    "    )\n",
    "    \n",
    "    # Use Qwen's chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze this question and return JSON only:\\n\\n{q}\"}\n",
    "    ]\n",
    "    \n",
    "    return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "FALLBACK = {\"is_child_context\": False, \"exclude_reason\": \"llm_error\", \"risk_flags\": [], \"quality\": \"low\"}\n",
    "\n",
    "def safe_json_parse(txt: str):\n",
    "    if not txt: return None\n",
    "    # Clean up common response patterns\n",
    "    txt = txt.strip()\n",
    "    # Remove markdown code blocks if present\n",
    "    txt = re.sub(r'^```json?\\s*', '', txt)\n",
    "    txt = re.sub(r'\\s*```$', '', txt)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except Exception:\n",
    "        # Try to extract JSON from the response\n",
    "        m = re.search(r'\\{[^{}]*\\}', txt, flags=re.S)\n",
    "        if m:\n",
    "            try: \n",
    "                return json.loads(m.group(0))\n",
    "            except Exception: \n",
    "                pass\n",
    "        return None\n",
    "\n",
    "# --- Batched classification on GPU ---\n",
    "def classify_batch(examples):\n",
    "    prompts = [make_prompt((e.get(\"questionText\") or \"\").strip()) for e in examples]\n",
    "    inputs = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,  # Low temperature for consistent JSON\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    gen_only = out[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    texts = tok.batch_decode(gen_only, skip_special_tokens=True)\n",
    "    \n",
    "    results = []\n",
    "    for t in texts:\n",
    "        data = safe_json_parse(t)\n",
    "        results.append(data if isinstance(data, dict) and \"is_child_context\" in data else FALLBACK.copy())\n",
    "    return results\n",
    "\n",
    "# --- Stage 2: LLM screening ---\n",
    "screened, stage2_rejected = [], []\n",
    "for i in tqdm(range(0, len(kept_stage1), BATCH_SIZE), desc=\"Stage 2 (Qwen screening)\"):\n",
    "    batch = kept_stage1[i:i+BATCH_SIZE]\n",
    "    labs = classify_batch(batch)\n",
    "    for ex, lab in zip(batch, labs):\n",
    "        ex2 = dict(ex); ex2[\"_screen\"] = lab\n",
    "        screened.append(ex2)\n",
    "        if not (\n",
    "            lab.get(\"is_child_context\") is True and\n",
    "            lab.get(\"quality\") in (\"high\",\"medium\") and\n",
    "            lab.get(\"exclude_reason\") in (\"none\", \"\", None)\n",
    "        ):\n",
    "            stage2_rejected.append(ex2)\n",
    "\n",
    "final = [\n",
    "    ex for ex in screened\n",
    "    if ex[\"_screen\"].get(\"is_child_context\") is True\n",
    "    and ex[\"_screen\"].get(\"quality\") in (\"high\",\"medium\")\n",
    "    and ex[\"_screen\"].get(\"exclude_reason\") in (\"none\", \"\", None)\n",
    "]\n",
    "print(f\"Stage 2 — kept: {len(final)} | rejected: {len(stage2_rejected)}\")\n",
    "\n",
    "# --- Save outputs ---\n",
    "def to_jsonl(path, rows):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "to_jsonl(os.path.join(OUT_DIR, \"rejected_stage1.jsonl\"), rejected_stage1)\n",
    "to_jsonl(os.path.join(OUT_DIR, \"rejected_stage2.jsonl\"), stage2_rejected)\n",
    "\n",
    "def scrub(s):\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"\\s+\",\" \",str(s)).strip()\n",
    "    s = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\",\"[redacted_email]\",s)\n",
    "    s = re.sub(r\"(https?://\\S+)\",\"[link]\",s)\n",
    "    s = re.sub(r\"\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\",\"[phone]\",s)\n",
    "    return s\n",
    "\n",
    "out_path = os.path.join(OUT_DIR, \"counselchat_child_subset_chat_screened.jsonl\")\n",
    "seen = set()\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in final:\n",
    "        q = scrub(r.get(\"questionText\",\"\")); a = scrub(r.get(\"answerText\",\"\"))\n",
    "        key = (q, a)\n",
    "        if not q or not a or key in seen: \n",
    "            continue\n",
    "        seen.add(key)\n",
    "        f.write(json.dumps({\"messages\":[\n",
    "            {\"role\":\"user\",\"content\": q},\n",
    "            {\"role\":\"assistant\",\"content\": a}\n",
    "        ]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {out_path} with {len(seen)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11007fe1",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
